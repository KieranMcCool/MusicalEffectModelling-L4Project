## 29/09/2017 - 3 Hours

* Began reading the first chapter of colah.github.io
* Watched YouTube Lectures on machine learning and neural networks

## 03/10/2017 - 2 Hours

* Read some of Section 1 of Nielson's book on machine learning

## 04/10/2017 - 2 Hours

* Prepared presentation on project for Advanced Geographies Presentation
* Installed PyTorch successfully
* Had trouble with Keras installation.

## 05/10/2017 - 2 Hours 

* Second project meeting
* Began reading "The Unreasonable Effectiveness of Recurrent Neural Networks."
* Spent time looking for tutorials on using PyTorch.

## 11/10/2017 - 2 Hours

* Moved Keras and PyTorch installations from using system Python and Pip to Anaconda virtual environment.
* Reread "The Unreasonable Effectiveness of Recurrent Neural Networks"
* Read Chris Olah's blog post on LSTM Netorks

## 12/10/2017 - 1 Hour

* Third project meeting
* Digitised minutes from today's meeting.

## 13/10/2017 - 2 Hours

* Started working on pipeline for generating test data
    - SciPy for generating wav files, Mrs Watson for processing them through the VST plugin.
    - Having trouble getting Mrs Watson to read the wav files generated.

## 17/10/2017 - 3 Hours

* Test data pipeline is mostly in place, just working out some bugs with MrsWatson.
	- MrsWatson processes the files and data looks to be there but no software can seem to read the resulting files...
	- Might try with MacOS/Windows instead of Linux as these appear to be better supported.
	- Choice of plugins also limited by poor support on Linux from VST developers.
    - Frustratingly, everything worked with only minor changes on Windows.
* Upon hearing the output from the VST plugins, I decided to move away from random frequency sweeps to those found within a guitars frequency range.
    - Researched what this range was and wrote code to replicate it.
* Looking at setting variables on effect with MrsWatson.

## 18/10/2017 - 1 Hour

* Worked through introduction PyTorch tutorials on their website.

## 19/10/2017 - 1 Hour

* Wrote up minutes from Meeting 4
    - Also fixed dates of meeting minutes, I had been forgetting to change them.

## 23/10/2017 - 1 Hours

* Cleaned up code for test data pipeline, almost ready to be finalised into a python CLI program.
* Read additional documentation for PyTorch

## 24/10/2017 - 2 Hours

* Followed PyTorch examples until intermediate
* Read some blogs about PyTorch
    - [](https://www.oreilly.com/ideas/why-ai-and-machine-learning-researchers-are-beginning-to-embrace-pytorch)

## 25/10/2017 - 2 Hours

* Adapting basic neural network from tutorials into something which can accept the wav data as read by SciPy
    - Reading in wav files with SciPy as Numpy arrays, converting to PyTorch tensors.

## 27/10/2017 - 1 Hour

* Wrote up minutes from meeting yesterday

## 30/10/2017 - 2 Hours

* Discovered bug in the pipeline involving MrsWatson
    - Picky about files/plugins and I'm not even sure it's working at all.
    - Tried a few different plugins, some work better than others, some crash all together
    - Inconsistency running even with the same command multiple times
* Tried RenderMan, an alternative to MrsWatson
    - Lots of dependencies
    - Couldn't work out how to compile it as some dependencies have updated their code and are no longer compatible but don't have legacy builds available for download.

## 31/10/2017 - 2 Hours

* Frustrated with Pipeline issues, decided to do what I could for now.
* Adapting examples for sound
    - Started writing code for sliding window approach.
    - Taking a wav file and splitting it into equal sized blocks, padding if necessary
    - This makes it easier to train the network
* Looked some more at DeepSound, might be useful to borrow some of their RNN code at a later date.

## 7/11/2017 - 5 Hours

* Minutes written up from last meeting (technically done on the day of the meeting, just digitised them)
* Code for generating training data is totally redone
    - Data is **far** more random than it was before.
    - Command line parameters added to allow easy test data generation
    - Randomisation made much simpler
    - Much more flexible options for creating test data, makes it easier to add even more variability should the need arise.
* Reaper command line instead of MrsWatson as issues from last time could not be fixed.

## 8/11/2017 - 4 Hours

* Created a bash script to create test data (generated by python script then automatically processed by reaper) with command line arguments to allow me to change duration and quantity of files.
* Finally got the network working and iterating over all the files
    - **REALLY EXCITING, LOSS IS ALREADY DECREASING SUBSTANTIALLY!!!**
    - Starts in the high millions and finishes in the low 100k, gets as low as 25k-30k on some files
    - Loss does increase when a new file is loaded but then quickly starts to decrease the more time it spends with the file. This might indicate overfitting a little bit? 
    - Some samples have absurdly high loss compared to others, but this seems to get less exagerated the more iterations the network is run for.
* Tried to make data even more random beyond the improvements made last night, this is still a work in progress.

##Â 10/11/2017 - 1 Hour

* Wrote up minutes from last meeting

## 14/11/2017 - 4 Hours

* Moved commonly used variables into globals.py
    - Things like sample rate etc
* Finished sliding window approach
    - Spacial awareness adjustable within globals.py
* Data represented as objects instead of list of lists
    - Easier memory management
    - Much easier to reason about the code.
    - Can be run on arbitrary files from path.
* Realistic network variables
    - Input magnitude = 64, Output Magnitude = 1
* Ran some initial experiments on reverb
    - 64 sample size didn't produce very good results
    - Increased to 200 for greater spacial awareness, which definitely improved things but training was painfully slow.

## 15/11/2017 - 1 Hours

* Set up a TS808 effects chain to train on
* Ran an initial experiment on TS808


## 16/11/2017 - 1 Hour

* Had project meeting
* Wrote up project meeting notes
* Looked at Conv1D in PyTorch
    - Can't seem to get it working with my data, wants 3D tensor, audio data is 1D

## 20/11/2017 - 1 Hour

* Looked at DeepSound and ProjectMagenta for potential example of conv1d applied to audio data.
    - Still not quite sure, DeepSound uses RNN instead and ProjectMagenta uses TensorFlow

## 22/11/2017 - 2 Hours

* Model saves at each checkpoint, not yet able to load a checkpoint.
* Verified that the training process is actually working by training the model to simply reproduce the input, this works as loss hits 0 very quickly.
* Ran current network on linear distortion
    - Overfitting seems to be a problem

## 23/11/2017 - 1 Hour

* Project meeting and wrote up minutes

## 27/11/2017 - 2 Hours

* Started working on getting convolutions added
    - Still struggling to understand what's going on
    - Getting errors saying expected 3D tensor when data.dim() reports we are using a 3D tensor?!?!?1

## 29/11/2017 - 3 Hours

* Frustrated, tried a change of tactics.
    - Instead of going for a 200x64x1 Tensor, let's just go for 1x1x64
    - Only focus on the 64 time slice
    - Seems to be working pretty well, loss is going lower than I've seen it before
* Network now going Conv1D(64) -> ReLU -> Conv1D(32) -> ReLU -> Conv1D(1) -> ReLU 
    - More convincing results than ever.

## 30/11/2017 - 1 Hour

* Wrote up minutes from meeting

## 5/12/2017 - 4 Hours

* Got batches of 200 working
    - Hoping to be able to randomly choose samples to train in a given batch
    - Also hoping to be able to run sequentially so I can run on output files 200 times faster.
* Some code cleanup
    - Model's forward function cleaned up by encapsulating multiple layers in Sequentials.
* Started moving run-time code into main.py so that net.py just contains the model and functions directly related to it.
    - Will be easier to reason about code base when every component is in its distinct location
    - Means I can look into adding command line arguments without ending up with a huge number of lines detracting from the pytorch specific code.

## 12/12/17 - 3 Hours

* Refactored data management code to extend PyTorch's DataLoader class, this makes it a lot faster preparing data as it is automatically done on multiple threads.
    - I noticed that GPU acceleration wasn't having the effect I thought it would with GPU usage only hitting 5% for a few seconds then dropping. I think this was an issue with my random sampling being slow. 
    - PyTorch's RandomSampler class is proving to be much faster. Hopefully I'll be able to saturate the GPU with work now.

## 15/12/17 - 2 Hours

* Tried out the LSTM layer of PyTorch.
    - Needed refactoring of dataloader as we were doing a 200x64x1 tensor, needed a 200x1x64 tensor.
    - Doesn't seem to learn very quickly, nor does it have much success - even with the basic distortions that we had decent success with previously.

## 16/12/17 - 2 Hours

* Tried convolutional and LSTM network with a short chorus effect. 
    - Doesn't seem to capture any properties of the effect...
    - Unsure if random sampling can give it the awareness of time based effects.

## 20/12/17 - 1 Hour

* Experimented with larger input vectors.
    - About a tenth of a seconds worth seems to produce a little bit of the chorus effect.
    - Seems to distort though. Unsure if this is perhaps due to lack of variation in volume in training data.

## 4/01/18 - 3 Hours

* Code cleanup in net.py
    - Moved a lot of the code into the model class so you can say model.train() rather than having the model be global and implicit
    - Will be useful when we move to main.py for controlling program
* CUDA is now toggled based on availability, will add a command line option to override in main.py
* Played with dilation values and fully connected architecture.
    - Still no closer to replicating chorus
    - Plan to try with RNN/LSTM next
    - Also plan to try add some more variety to test data in case poor results are a result of test data being drastically different to real world data.

## 7/01/18 - 2 Hours

* Moved network architecture into subclasses of model so they can be selected and changed easily.
    - Sick of making changes to architecture and forgetting what the originals were! :(

## 14/01/18 - 2 Hours

* Experimented with LSTM networks, with little success.
    - Tried single LSTM, various different input vector sizes. Didn't perform as well as convolutional network for any effect.
    - Tried stacking LSTMs, still no joy
    - Tried LSTMs -> Linear Layers and Linear Layers -> LSTMs - Still no results
    - Beginning to worry that LSTMS might not be practical for task?

## 17/01/18 - 2 Hours

* Worked on pipeline to allow easier inclusion of real world music / additional sound files to investigate potential that overfitting was the issue.
    - Now a directory in the pipeline source where pre-prepared audio data can be stored for inclusion. 
    - Also added one for test files in case I want to use multiple test files for whatever reason.
* Cleaned up generate script a bit to allow easier change from MacOS to Linux as required.
* Trained some models on real world music data.
    - Doesn't seem to help with my issue with the LSTMs but seems to be the way to go for the convolutional network.

## 24/01/18 - 3 Hours

* Went back to basic with LSTM trying to see if it could replicate the input by training on identical files since nothing was working with chorus etc.
    - Single LSTM layer with single sample input.
    - Single output which should be the same as the input.
    - Doesn't learn that either...
* Trained some convolutional models on distortion for ABY testing.

## 24/01/18 - 4 Hours

* Spent some time prettying up the generate script
    - Uses functions instead of duplicating code with different directories.
    - Much shorter and much more maintainable
* Merged the pipeline and neural network folders
    - Was keeping them apart for sake of not having too many source files in one folder but I think it's fine
    - Is far more convenient
* Added some debug code for seeing what the network is doing internally, can be disabled as needed.
    - Needed to diagnose issues with LSTM.
    - Lets me see what the target and prediction is at a given iteration
    - Also allows me to see what the shape of the input and output vectors are just for troubleshooting.
* Played some more with LSTMs, experimenting with batch sizes and properties on chorus.
    - Still distorting...
    - Trained for 100,000 iterations outputting ever 10,000
    - Some got clos-ish
* Started writing my random sequential sampler
    - Samples n items sequentially from a random index
    - Seems to work....
* Now LSTM seems to just output the same as input!!!?????

## 25/01/18 - 4 Hours

* Wrote up minutes from last two meetings
* Changed LSTM to have much larger hidden size and number of layers
* Spent some time training the network to evaluate the effect of training.
    - Tried 200 Hidden features, 6 layers, doesn't work. Produced distorted sound
    - Tried 500 hidden features, 6 layers, doesn't work. Now reproduces input sound
    - Tried 800 Hidden features, 6 layers, wavey, worbly effect present but way too much gain.
    - Tried same again but with a bigger fully connected layer (LSTM -> reLU -> 800-> reLU -> 400 -> reLU ->200->1)

## 01/02/18 - 3 Hours

* Read up on some similar projects using LSTMs for time series prediction.
	- Example code on GitHub
* Worked on adapting this to my project
* Got it working but LSTM Cell is so slow to train that I can't actually get an output from it.
	- No CUDA support :(

## 03/02/18 - 2 Hours

* Read up on Nebula VST and researched what it can/can't do as well as how it does it.
	- No information on how present version works due to patents.

## 06/02/18 - 2 Hours

* Researched and played with a bunch of VSTs for finalising my list of effects.
	- Grouping as Filters, Distortion/Boost, and Time-Based
	- Noise gate, TS808, Fuzz, Compressor, Reverb, Chorus, and Delay.
	- 3 Presets for first 5, 6 for last two.
